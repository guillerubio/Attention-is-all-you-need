{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_proc.ipynb \n",
    "### Made by William Rubio on June 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-06-14 13:31:36--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: 'data/input.txt.2'\n",
      "\n",
      "input.txt.2         100%[===================>]   1.06M   612KB/s    in 1.8s    \n",
      "\n",
      "2023-06-14 13:31:39 (612 KB/s) - 'data/input.txt.2' saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "!wget -P data/ https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset and save it in a variable\n",
    "with open('data/input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Inspect the characters that are present in the dataset\n",
    "chars = sorted(list(set(text)))\n",
    "print(chars)\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "Tokenize input text: Covert raw text as a string to a sequence of integers according to a vocabulary of possible elements. \n",
    "Google uses SentencePiece, it's a sub-word tokenizer, ++ Vocabulaty == ++ Tokens == -- Decode array == More reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, '\\n'], [1, ' '], [2, '!'], [3, '$'], [4, '&'], [5, \"'\"], [6, ','], [7, '-'], [8, '.'], [9, '3'], [10, ':'], [11, ';'], [12, '?'], [13, 'A'], [14, 'B'], [15, 'C'], [16, 'D'], [17, 'E'], [18, 'F'], [19, 'G'], [20, 'H'], [21, 'I'], [22, 'J'], [23, 'K'], [24, 'L'], [25, 'M'], [26, 'N'], [27, 'O'], [28, 'P'], [29, 'Q'], [30, 'R'], [31, 'S'], [32, 'T'], [33, 'U'], [34, 'V'], [35, 'W'], [36, 'X'], [37, 'Y'], [38, 'Z'], [39, 'a'], [40, 'b'], [41, 'c'], [42, 'd'], [43, 'e'], [44, 'f'], [45, 'g'], [46, 'h'], [47, 'i'], [48, 'j'], [49, 'k'], [50, 'l'], [51, 'm'], [52, 'n'], [53, 'o'], [54, 'p'], [55, 'q'], [56, 'r'], [57, 's'], [58, 't'], [59, 'u'], [60, 'v'], [61, 'w'], [62, 'x'], [63, 'y'], [64, 'z']]\n"
     ]
    }
   ],
   "source": [
    "stoi = []\n",
    "i = 0\n",
    "for character in chars:\n",
    "    stoi.append([i, character])\n",
    "    i += 1\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and decode functions\n",
    "def encode(string):\n",
    "    ans = []\n",
    "    for character in string:\n",
    "        for tuple in stoi:\n",
    "            if tuple[1] == character:\n",
    "                ans.append(tuple[0])\n",
    "                break\n",
    "    return ans\n",
    "\n",
    "def decode(arr):\n",
    "    ans = \"\"\n",
    "    for num in arr:\n",
    "        for tuple in stoi:\n",
    "            if tuple[0] == num:\n",
    "                ans+=tuple[1]\n",
    "                break\n",
    "    return ans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# Encode dataset\n",
    "import torch as pt\n",
    "data = pt.tensor(encode(text), dtype=pt.long)\n",
    "print(data.shape)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch prep\n",
    "Computationally, it is not efficient to train with all the data at once. Because of this, we dide it in chunks (blocks). The reason why this is way more efficioent is because machines are good at concurrent processing, and we should be using all cores in a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18]) -> tensor(47)\n",
      "tensor([18, 47]) -> tensor(56)\n",
      "tensor([18, 47, 56]) -> tensor(57)\n",
      "tensor([18, 47, 56, 57]) -> tensor(58)\n",
      "tensor([18, 47, 56, 57, 58]) -> tensor(1)\n",
      "tensor([18, 47, 56, 57, 58,  1]) -> tensor(15)\n",
      "tensor([18, 47, 56, 57, 58,  1, 15]) -> tensor(47)\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47]) -> tensor(58)\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "    print(context, \"->\", target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick explanation of the code below: each row of the input x is a chunk of the training set. Each row of the target y is the number it should predict given the input in the corresponding placement of x. For example, looking at the first row and column element of x and y, we can see that 24 is likely to produce 43, 24 + 43 is likely to produce 58. Etc, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets: \n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # Sequences to process in parallel\n",
    "block_size = 8 # Maximum context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\" Generate a small batch of data of inputs x and targets y\"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # Generate batch_size random integers from 0 to len(data)-block_size, this will be the random offset for each sequence in the batch\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # Stack the sequences (they have different lengths)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # Shifted version of the sequences for the targets\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"inputs: \")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"targets: \")\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model training\n",
    "We will start with the pytorch bigram language model. Quick definition: A bigram language model is a word prediction mathematical model that predicts the next word in a sentence based on the previous words. It achieves by analyzing patterns observed in large collections of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # The token embedding table is lookup table that maps each word or token to its corresponding vector representation\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
